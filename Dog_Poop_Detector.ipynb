{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4gTz1gG_GmG",
        "outputId": "adc65a1f-be58-4e94-9fab-c8b7a96b7944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 616 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 34.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 56.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 840 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 38.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 27.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 74.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 45 kB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 58.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 59.0 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amAnQ1Q8_KCY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APl54P6T_U2L"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/download.tensorflow.org/data/android_figurine.zip\n",
        "!unzip -q android_figurine.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMSao-1piBZu",
        "outputId": "052a2d13-547f-4e77-fa49-8eb8f54371b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KMjn0MEik8Q",
        "outputId": "c17a799a-4fa0-47e1-9cea-88b5e68c376b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalidate\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls /content/drive/MyDrive/dog_pooping/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "677vggfp_fg3"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/drive/MyDrive/dog_pooping/train',\n",
        "    '/content/drive/MyDrive/dog_pooping/train',\n",
        "    ['dog_pooping']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/drive/MyDrive/dog_pooping/validate',\n",
        "    '/content/drive/MyDrive/dog_pooping/validate',\n",
        "    ['dog_pooping']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Nwk-NT_prE"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3owaFjz8_tpW",
        "outputId": "8aa36f00-8a84-40ca-abe3-9f569b793062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "9/9 [==============================] - 326s 28s/step - det_loss: 1.7466 - cls_loss: 1.1508 - box_loss: 0.0119 - reg_l2_loss: 0.1078 - loss: 1.8544 - learning_rate: 0.0065 - gradient_norm: 1.5872 - val_det_loss: 1.5609 - val_cls_loss: 1.0738 - val_box_loss: 0.0097 - val_reg_l2_loss: 0.1078 - val_loss: 1.6687\n",
            "Epoch 2/20\n",
            "9/9 [==============================] - 241s 27s/step - det_loss: 1.5246 - cls_loss: 1.0482 - box_loss: 0.0095 - reg_l2_loss: 0.1078 - loss: 1.6324 - learning_rate: 0.0049 - gradient_norm: 2.1364 - val_det_loss: 1.2700 - val_cls_loss: 0.8509 - val_box_loss: 0.0084 - val_reg_l2_loss: 0.1078 - val_loss: 1.3778\n",
            "Epoch 3/20\n",
            "9/9 [==============================] - 248s 28s/step - det_loss: 1.2406 - cls_loss: 0.8459 - box_loss: 0.0079 - reg_l2_loss: 0.1078 - loss: 1.3484 - learning_rate: 0.0048 - gradient_norm: 3.0759 - val_det_loss: 0.7263 - val_cls_loss: 0.4367 - val_box_loss: 0.0058 - val_reg_l2_loss: 0.1078 - val_loss: 0.8341\n",
            "Epoch 4/20\n",
            "9/9 [==============================] - 241s 27s/step - det_loss: 0.8919 - cls_loss: 0.5491 - box_loss: 0.0069 - reg_l2_loss: 0.1078 - loss: 0.9997 - learning_rate: 0.0046 - gradient_norm: 3.6638 - val_det_loss: 0.9598 - val_cls_loss: 0.7169 - val_box_loss: 0.0049 - val_reg_l2_loss: 0.1078 - val_loss: 1.0676\n",
            "Epoch 5/20\n",
            "9/9 [==============================] - 265s 30s/step - det_loss: 0.6466 - cls_loss: 0.4077 - box_loss: 0.0048 - reg_l2_loss: 0.1078 - loss: 0.7544 - learning_rate: 0.0043 - gradient_norm: 3.1439 - val_det_loss: 0.4771 - val_cls_loss: 0.2563 - val_box_loss: 0.0044 - val_reg_l2_loss: 0.1078 - val_loss: 0.5850\n",
            "Epoch 6/20\n",
            "9/9 [==============================] - 245s 27s/step - det_loss: 0.5488 - cls_loss: 0.3449 - box_loss: 0.0041 - reg_l2_loss: 0.1078 - loss: 0.6567 - learning_rate: 0.0040 - gradient_norm: 2.8085 - val_det_loss: 0.3666 - val_cls_loss: 0.2092 - val_box_loss: 0.0031 - val_reg_l2_loss: 0.1078 - val_loss: 0.4744\n",
            "Epoch 7/20\n",
            "9/9 [==============================] - 242s 27s/step - det_loss: 0.4997 - cls_loss: 0.3268 - box_loss: 0.0035 - reg_l2_loss: 0.1078 - loss: 0.6075 - learning_rate: 0.0037 - gradient_norm: 2.9880 - val_det_loss: 0.3653 - val_cls_loss: 0.2089 - val_box_loss: 0.0031 - val_reg_l2_loss: 0.1078 - val_loss: 0.4731\n",
            "Epoch 8/20\n",
            "9/9 [==============================] - 245s 27s/step - det_loss: 0.4292 - cls_loss: 0.2806 - box_loss: 0.0030 - reg_l2_loss: 0.1078 - loss: 0.5370 - learning_rate: 0.0033 - gradient_norm: 2.4868 - val_det_loss: 0.3115 - val_cls_loss: 0.1880 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1078 - val_loss: 0.4193\n",
            "Epoch 9/20\n",
            "9/9 [==============================] - 243s 27s/step - det_loss: 0.4587 - cls_loss: 0.3237 - box_loss: 0.0027 - reg_l2_loss: 0.1078 - loss: 0.5665 - learning_rate: 0.0029 - gradient_norm: 4.2642 - val_det_loss: 0.3160 - val_cls_loss: 0.2104 - val_box_loss: 0.0021 - val_reg_l2_loss: 0.1078 - val_loss: 0.4238\n",
            "Epoch 10/20\n",
            "9/9 [==============================] - 259s 29s/step - det_loss: 0.4180 - cls_loss: 0.3005 - box_loss: 0.0023 - reg_l2_loss: 0.1078 - loss: 0.5258 - learning_rate: 0.0025 - gradient_norm: 3.3626 - val_det_loss: 0.2826 - val_cls_loss: 0.1990 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.1078 - val_loss: 0.3904\n",
            "Epoch 11/20\n",
            "9/9 [==============================] - 240s 27s/step - det_loss: 0.3699 - cls_loss: 0.2688 - box_loss: 0.0020 - reg_l2_loss: 0.1078 - loss: 0.4777 - learning_rate: 0.0021 - gradient_norm: 1.9787 - val_det_loss: 0.2620 - val_cls_loss: 0.1866 - val_box_loss: 0.0015 - val_reg_l2_loss: 0.1078 - val_loss: 0.3698\n",
            "Epoch 12/20\n",
            "9/9 [==============================] - 242s 27s/step - det_loss: 0.3663 - cls_loss: 0.2619 - box_loss: 0.0021 - reg_l2_loss: 0.1078 - loss: 0.4741 - learning_rate: 0.0017 - gradient_norm: 3.2405 - val_det_loss: 0.2617 - val_cls_loss: 0.1928 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.1078 - val_loss: 0.3696\n",
            "Epoch 13/20\n",
            "9/9 [==============================] - 247s 28s/step - det_loss: 0.3696 - cls_loss: 0.2701 - box_loss: 0.0020 - reg_l2_loss: 0.1078 - loss: 0.4775 - learning_rate: 0.0013 - gradient_norm: 2.7564 - val_det_loss: 0.2616 - val_cls_loss: 0.2006 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.1078 - val_loss: 0.3695\n",
            "Epoch 14/20\n",
            "9/9 [==============================] - 242s 27s/step - det_loss: 0.3451 - cls_loss: 0.2536 - box_loss: 0.0018 - reg_l2_loss: 0.1078 - loss: 0.4529 - learning_rate: 9.7001e-04 - gradient_norm: 2.5542 - val_det_loss: 0.2327 - val_cls_loss: 0.1774 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.1078 - val_loss: 0.3405\n",
            "Epoch 15/20\n",
            "9/9 [==============================] - 257s 29s/step - det_loss: 0.3083 - cls_loss: 0.2297 - box_loss: 0.0016 - reg_l2_loss: 0.1078 - loss: 0.4162 - learning_rate: 6.6610e-04 - gradient_norm: 1.9941 - val_det_loss: 0.2297 - val_cls_loss: 0.1761 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.1078 - val_loss: 0.3376\n",
            "Epoch 16/20\n",
            "9/9 [==============================] - 240s 27s/step - det_loss: 0.3448 - cls_loss: 0.2562 - box_loss: 0.0018 - reg_l2_loss: 0.1078 - loss: 0.4526 - learning_rate: 4.1222e-04 - gradient_norm: 1.9115 - val_det_loss: 0.2249 - val_cls_loss: 0.1752 - val_box_loss: 9.9526e-04 - val_reg_l2_loss: 0.1078 - val_loss: 0.3328\n",
            "Epoch 17/20\n",
            "9/9 [==============================] - 250s 28s/step - det_loss: 0.3253 - cls_loss: 0.2441 - box_loss: 0.0016 - reg_l2_loss: 0.1078 - loss: 0.4331 - learning_rate: 2.1528e-04 - gradient_norm: 2.0171 - val_det_loss: 0.2234 - val_cls_loss: 0.1754 - val_box_loss: 9.5966e-04 - val_reg_l2_loss: 0.1078 - val_loss: 0.3313\n",
            "Epoch 18/20\n",
            "9/9 [==============================] - 251s 28s/step - det_loss: 0.2964 - cls_loss: 0.2270 - box_loss: 0.0014 - reg_l2_loss: 0.1078 - loss: 0.4042 - learning_rate: 8.0670e-05 - gradient_norm: 1.5256 - val_det_loss: 0.2215 - val_cls_loss: 0.1748 - val_box_loss: 9.3405e-04 - val_reg_l2_loss: 0.1078 - val_loss: 0.3293\n",
            "Epoch 19/20\n",
            "9/9 [==============================] - 244s 27s/step - det_loss: 0.3011 - cls_loss: 0.2378 - box_loss: 0.0013 - reg_l2_loss: 0.1078 - loss: 0.4089 - learning_rate: 1.2048e-05 - gradient_norm: 1.6715 - val_det_loss: 0.2210 - val_cls_loss: 0.1747 - val_box_loss: 9.2468e-04 - val_reg_l2_loss: 0.1078 - val_loss: 0.3288\n",
            "Epoch 20/20\n",
            "9/9 [==============================] - 260s 29s/step - det_loss: 0.3109 - cls_loss: 0.2310 - box_loss: 0.0016 - reg_l2_loss: 0.1078 - loss: 0.4187 - learning_rate: 1.1292e-05 - gradient_norm: 2.4195 - val_det_loss: 0.2200 - val_cls_loss: 0.1744 - val_box_loss: 9.1158e-04 - val_reg_l2_loss: 0.1078 - val_loss: 0.3278\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=20, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRLYpuy-_2sW"
      },
      "outputs": [],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl6ySLo3_9TJ"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='/content/drive/MyDrive/', tflite_filename='dog_pooping.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JealJb2qAB-X"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('/content/drive/MyDrive/dog_pooping.tflite', val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmN9G1k3AHWu"
      },
      "outputs": [],
      "source": [
        "# Download the TFLite model to your local computer.\n",
        "from google.colab import files\n",
        "files.download('android.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfFH9oUYAe8m"
      },
      "outputs": [],
      "source": [
        "#@title Load the trained TFLite model and define some visualization functions\n",
        "\n",
        "#@markdown This code comes from the TFLite Object Detection [Raspberry Pi sample](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi).\n",
        "\n",
        "import platform\n",
        "from typing import List, NamedTuple\n",
        "import json\n",
        "\n",
        "import cv2\n",
        "\n",
        "Interpreter = tf.lite.Interpreter\n",
        "load_delegate = tf.lite.experimental.load_delegate\n",
        "\n",
        "# pylint: enable=g-import-not-at-top\n",
        "\n",
        "\n",
        "class ObjectDetectorOptions(NamedTuple):\n",
        "  \"\"\"A config to initialize an object detector.\"\"\"\n",
        "\n",
        "  enable_edgetpu: bool = False\n",
        "  \"\"\"Enable the model to run on EdgeTPU.\"\"\"\n",
        "\n",
        "  label_allow_list: List[str] = None\n",
        "  \"\"\"The optional allow list of labels.\"\"\"\n",
        "\n",
        "  label_deny_list: List[str] = None\n",
        "  \"\"\"The optional deny list of labels.\"\"\"\n",
        "\n",
        "  max_results: int = -1\n",
        "  \"\"\"The maximum number of top-scored detection results to return.\"\"\"\n",
        "\n",
        "  num_threads: int = 1\n",
        "  \"\"\"The number of CPU threads to be used.\"\"\"\n",
        "\n",
        "  score_threshold: float = 0.0\n",
        "  \"\"\"The score threshold of detection results to return.\"\"\"\n",
        "\n",
        "\n",
        "class Rect(NamedTuple):\n",
        "  \"\"\"A rectangle in 2D space.\"\"\"\n",
        "  left: float\n",
        "  top: float\n",
        "  right: float\n",
        "  bottom: float\n",
        "\n",
        "\n",
        "class Category(NamedTuple):\n",
        "  \"\"\"A result of a classification task.\"\"\"\n",
        "  label: str\n",
        "  score: float\n",
        "  index: int\n",
        "\n",
        "\n",
        "class Detection(NamedTuple):\n",
        "  \"\"\"A detected object as the result of an ObjectDetector.\"\"\"\n",
        "  bounding_box: Rect\n",
        "  categories: List[Category]\n",
        "\n",
        "\n",
        "def edgetpu_lib_name():\n",
        "  \"\"\"Returns the library name of EdgeTPU in the current platform.\"\"\"\n",
        "  return {\n",
        "      'Darwin': 'libedgetpu.1.dylib',\n",
        "      'Linux': 'libedgetpu.so.1',\n",
        "      'Windows': 'edgetpu.dll',\n",
        "  }.get(platform.system(), None)\n",
        "\n",
        "\n",
        "class ObjectDetector:\n",
        "  \"\"\"A wrapper class for a TFLite object detection model.\"\"\"\n",
        "\n",
        "  _OUTPUT_LOCATION_NAME = 'location'\n",
        "  _OUTPUT_CATEGORY_NAME = 'category'\n",
        "  _OUTPUT_SCORE_NAME = 'score'\n",
        "  _OUTPUT_NUMBER_NAME = 'number of detections'\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_path: str,\n",
        "      options: ObjectDetectorOptions = ObjectDetectorOptions()\n",
        "  ) -> None:\n",
        "    \"\"\"Initialize a TFLite object detection model.\n",
        "    Args:\n",
        "        model_path: Path to the TFLite model.\n",
        "        options: The config to initialize an object detector. (Optional)\n",
        "    Raises:\n",
        "        ValueError: If the TFLite model is invalid.\n",
        "        OSError: If the current OS isn't supported by EdgeTPU.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load metadata from model.\n",
        "    displayer = metadata.MetadataDisplayer.with_model_file(model_path)\n",
        "\n",
        "    # Save model metadata for preprocessing later.\n",
        "    model_metadata = json.loads(displayer.get_metadata_json())\n",
        "    process_units = model_metadata['subgraph_metadata'][0]['input_tensor_metadata'][0]['process_units']\n",
        "    mean = 0.0\n",
        "    std = 1.0\n",
        "    for option in process_units:\n",
        "      if option['options_type'] == 'NormalizationOptions':\n",
        "        mean = option['options']['mean'][0]\n",
        "        std = option['options']['std'][0]\n",
        "    self._mean = mean\n",
        "    self._std = std\n",
        "\n",
        "    # Load label list from metadata.\n",
        "    file_name = displayer.get_packed_associated_file_list()[0]\n",
        "    label_map_file = displayer.get_associated_file_buffer(file_name).decode()\n",
        "    label_list = list(filter(lambda x: len(x) > 0, label_map_file.splitlines()))\n",
        "    self._label_list = label_list\n",
        "\n",
        "    # Initialize TFLite model.\n",
        "    if options.enable_edgetpu:\n",
        "      if edgetpu_lib_name() is None:\n",
        "        raise OSError(\"The current OS isn't supported by Coral EdgeTPU.\")\n",
        "      interpreter = Interpreter(\n",
        "          model_path=model_path,\n",
        "          experimental_delegates=[load_delegate(edgetpu_lib_name())],\n",
        "          num_threads=options.num_threads)\n",
        "    else:\n",
        "      interpreter = Interpreter(\n",
        "          model_path=model_path, num_threads=options.num_threads)\n",
        "\n",
        "    interpreter.allocate_tensors()\n",
        "    input_detail = interpreter.get_input_details()[0]\n",
        "\n",
        "    # From TensorFlow 2.6, the order of the outputs become undefined.\n",
        "    # Therefore we need to sort the tensor indices of TFLite outputs and to know\n",
        "    # exactly the meaning of each output tensor. For example, if\n",
        "    # output indices are [601, 599, 598, 600], tensor names and indices aligned\n",
        "    # are:\n",
        "    #   - location: 598\n",
        "    #   - category: 599\n",
        "    #   - score: 600\n",
        "    #   - detection_count: 601\n",
        "    # because of the op's ports of TFLITE_DETECTION_POST_PROCESS\n",
        "    # (https://github.com/tensorflow/tensorflow/blob/a4fe268ea084e7d323133ed7b986e0ae259a2bc7/tensorflow/lite/kernels/detection_postprocess.cc#L47-L50).\n",
        "    sorted_output_indices = sorted(\n",
        "        [output['index'] for output in interpreter.get_output_details()])\n",
        "    self._output_indices = {\n",
        "        self._OUTPUT_LOCATION_NAME: sorted_output_indices[0],\n",
        "        self._OUTPUT_CATEGORY_NAME: sorted_output_indices[1],\n",
        "        self._OUTPUT_SCORE_NAME: sorted_output_indices[2],\n",
        "        self._OUTPUT_NUMBER_NAME: sorted_output_indices[3],\n",
        "    }\n",
        "\n",
        "    self._input_size = input_detail['shape'][2], input_detail['shape'][1]\n",
        "    self._is_quantized_input = input_detail['dtype'] == np.uint8\n",
        "    self._interpreter = interpreter\n",
        "    self._options = options\n",
        "\n",
        "  def detect(self, input_image: np.ndarray) -> List[Detection]:\n",
        "    \"\"\"Run detection on an input image.\n",
        "    Args:\n",
        "        input_image: A [height, width, 3] RGB image. Note that height and width\n",
        "          can be anything since the image will be immediately resized according\n",
        "          to the needs of the model within this function.\n",
        "    Returns:\n",
        "        A Person instance.\n",
        "    \"\"\"\n",
        "    image_height, image_width, _ = input_image.shape\n",
        "\n",
        "    input_tensor = self._preprocess(input_image)\n",
        "\n",
        "    self._set_input_tensor(input_tensor)\n",
        "    self._interpreter.invoke()\n",
        "\n",
        "    # Get all output details\n",
        "    boxes = self._get_output_tensor(self._OUTPUT_LOCATION_NAME)\n",
        "    classes = self._get_output_tensor(self._OUTPUT_CATEGORY_NAME)\n",
        "    scores = self._get_output_tensor(self._OUTPUT_SCORE_NAME)\n",
        "    count = int(self._get_output_tensor(self._OUTPUT_NUMBER_NAME))\n",
        "\n",
        "    return self._postprocess(boxes, classes, scores, count, image_width,\n",
        "                             image_height)\n",
        "\n",
        "  def _preprocess(self, input_image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Preprocess the input image as required by the TFLite model.\"\"\"\n",
        "\n",
        "    # Resize the input\n",
        "    input_tensor = cv2.resize(input_image, self._input_size)\n",
        "\n",
        "    # Normalize the input if it's a float model (aka. not quantized)\n",
        "    if not self._is_quantized_input:\n",
        "      input_tensor = (np.float32(input_tensor) - self._mean) / self._std\n",
        "\n",
        "    # Add batch dimension\n",
        "    input_tensor = np.expand_dims(input_tensor, axis=0)\n",
        "\n",
        "    return input_tensor\n",
        "\n",
        "  def _set_input_tensor(self, image):\n",
        "    \"\"\"Sets the input tensor.\"\"\"\n",
        "    tensor_index = self._interpreter.get_input_details()[0]['index']\n",
        "    input_tensor = self._interpreter.tensor(tensor_index)()[0]\n",
        "    input_tensor[:, :] = image\n",
        "\n",
        "  def _get_output_tensor(self, name):\n",
        "    \"\"\"Returns the output tensor at the given index.\"\"\"\n",
        "    output_index = self._output_indices[name]\n",
        "    tensor = np.squeeze(self._interpreter.get_tensor(output_index))\n",
        "    return tensor\n",
        "\n",
        "  def _postprocess(self, boxes: np.ndarray, classes: np.ndarray,\n",
        "                   scores: np.ndarray, count: int, image_width: int,\n",
        "                   image_height: int) -> List[Detection]:\n",
        "    \"\"\"Post-process the output of TFLite model into a list of Detection objects.\n",
        "    Args:\n",
        "        boxes: Bounding boxes of detected objects from the TFLite model.\n",
        "        classes: Class index of the detected objects from the TFLite model.\n",
        "        scores: Confidence scores of the detected objects from the TFLite model.\n",
        "        count: Number of detected objects from the TFLite model.\n",
        "        image_width: Width of the input image.\n",
        "        image_height: Height of the input image.\n",
        "    Returns:\n",
        "        A list of Detection objects detected by the TFLite model.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Parse the model output into a list of Detection entities.\n",
        "    for i in range(count):\n",
        "      if scores[i] >= self._options.score_threshold:\n",
        "        y_min, x_min, y_max, x_max = boxes[i]\n",
        "        bounding_box = Rect(\n",
        "            top=int(y_min * image_height),\n",
        "            left=int(x_min * image_width),\n",
        "            bottom=int(y_max * image_height),\n",
        "            right=int(x_max * image_width))\n",
        "        class_id = int(classes[i])\n",
        "        category = Category(\n",
        "            score=scores[i],\n",
        "            label=self._label_list[class_id],  # 0 is reserved for background\n",
        "            index=class_id)\n",
        "        result = Detection(bounding_box=bounding_box, categories=[category])\n",
        "        results.append(result)\n",
        "\n",
        "    # Sort detection results by score ascending\n",
        "    sorted_results = sorted(\n",
        "        results,\n",
        "        key=lambda detection: detection.categories[0].score,\n",
        "        reverse=True)\n",
        "\n",
        "    # Filter out detections in deny list\n",
        "    filtered_results = sorted_results\n",
        "    if self._options.label_deny_list is not None:\n",
        "      filtered_results = list(\n",
        "          filter(\n",
        "              lambda detection: detection.categories[0].label not in self.\n",
        "              _options.label_deny_list, filtered_results))\n",
        "\n",
        "    # Keep only detections in allow list\n",
        "    if self._options.label_allow_list is not None:\n",
        "      filtered_results = list(\n",
        "          filter(\n",
        "              lambda detection: detection.categories[0].label in self._options.\n",
        "              label_allow_list, filtered_results))\n",
        "\n",
        "    # Only return maximum of max_results detection.\n",
        "    if self._options.max_results > 0:\n",
        "      result_count = min(len(filtered_results), self._options.max_results)\n",
        "      filtered_results = filtered_results[:result_count]\n",
        "\n",
        "    return filtered_results\n",
        "\n",
        "\n",
        "_MARGIN = 10  # pixels\n",
        "_ROW_SIZE = 10  # pixels\n",
        "_FONT_SIZE = 1\n",
        "_FONT_THICKNESS = 1\n",
        "_TEXT_COLOR = (0, 0, 255)  # red\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    image: np.ndarray,\n",
        "    detections: List[Detection],\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Draws bounding boxes on the input image and return it.\n",
        "  Args:\n",
        "    image: The input RGB image.\n",
        "    detections: The list of all \"Detection\" entities to be visualize.\n",
        "  Returns:\n",
        "    Image with bounding boxes.\n",
        "  \"\"\"\n",
        "  for detection in detections:\n",
        "    # Draw bounding_box\n",
        "    start_point = detection.bounding_box.left, detection.bounding_box.top\n",
        "    end_point = detection.bounding_box.right, detection.bounding_box.bottom\n",
        "    cv2.rectangle(image, start_point, end_point, _TEXT_COLOR, 3)\n",
        "\n",
        "    # Draw label and score\n",
        "    category = detection.categories[0]\n",
        "    class_name = category.label\n",
        "    probability = round(category.score, 2)\n",
        "    result_text = class_name + ' (' + str(probability) + ')'\n",
        "    text_location = (_MARGIN + detection.bounding_box.left,\n",
        "                     _MARGIN + _ROW_SIZE + detection.bounding_box.top)\n",
        "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
        "                _FONT_SIZE, _TEXT_COLOR, _FONT_THICKNESS)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqx4LmWJA-45"
      },
      "outputs": [],
      "source": [
        "#@title Run object detection and show the detection results\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "INPUT_IMAGE_URL = \"http://download.tensorflow.org/example_images/android_figurine.jpg\" #@param {type:\"string\"}\n",
        "DETECTION_THRESHOLD = 0.5 #@param {type:\"number\"}\n",
        "TFLITE_MODEL_PATH = \"android.tflite\" #@param {type:\"string\"}\n",
        "\n",
        "TEMP_FILE = '/tmp/image.png'\n",
        "\n",
        "!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
        "image = Image.open(TEMP_FILE).convert('RGB')\n",
        "image.thumbnail((512, 512), Image.ANTIALIAS)\n",
        "image_np = np.asarray(image)\n",
        "\n",
        "# Load the TFLite model\n",
        "options = ObjectDetectorOptions(\n",
        "      num_threads=4,\n",
        "      score_threshold=DETECTION_THRESHOLD,\n",
        ")\n",
        "detector = ObjectDetector(model_path=TFLITE_MODEL_PATH, options=options)\n",
        "\n",
        "# Run object detection estimation using the model.\n",
        "detections = detector.detect(image_np)\n",
        "\n",
        "# Draw keypoints and edges on input image\n",
        "image_np = visualize(image_np, detections)\n",
        "\n",
        "# Show the detection result\n",
        "Image.fromarray(image_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f10iEddvBFhK"
      },
      "outputs": [],
      "source": [
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install edgetpu-compiler"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
